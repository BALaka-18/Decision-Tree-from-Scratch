{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating my own Decision Tree using the concept of Gini Impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Green' '3' 'Apple']\n",
      " ['Yellow' '3' 'Apple']\n",
      " ['Red' '1' 'Grape']\n",
      " ['Red' '1' 'Grape']\n",
      " ['Yellow' '3' 'Lemon']]\n",
      "\n",
      "     Color Diameter Fruit_label\n",
      "0   Green        3       Apple\n",
      "1  Yellow        3       Apple\n",
      "2     Red        1       Grape\n",
      "3     Red        1       Grape\n",
      "4  Yellow        3       Lemon\n",
      "\n",
      " 5\n"
     ]
    }
   ],
   "source": [
    "# Creating a toy dataset of types of fruits in the order [Color, Diameter, Label]\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# toy_dataset = [['Red', 1, 'Grape'], ['Yellow', 3, 'Lemon'], ['Red', 1, 'Grape'], ['Yellow', 3, 'Apple'], ['Yellow', 3, 'Lemon'], ['Green', 3, 'Apple'], ['Red', 1, 'Grape'], ['Yellow', 3, 'Lemon']]\n",
    "toy_dataset = [['Green', 3, 'Apple'], ['Yellow', 3, 'Apple'], ['Red', 1, 'Grape'], ['Red', 1, 'Grape'], ['Yellow', 3, 'Lemon']]\n",
    "data = np.array(toy_dataset)\n",
    "print(data)\n",
    "d = pd.DataFrame({'Color':data[:, 0], 'Diameter':data[:, 1], 'Fruit_label':data[:, 2]})\n",
    "print(\"\\n\",d)\n",
    "\n",
    "print(\"\\n\", len(toy_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning labels to each column\n",
    "head = ['Color', 'Diameter', 'Fruit_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1, 3}\n"
     ]
    }
   ],
   "source": [
    "def unique_values_in_a_col(rows, col):\n",
    "    return set([row[col] for row in rows])\n",
    "\n",
    "print(unique_values_in_a_col(toy_dataset, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Apple': 2, 'Grape': 2, 'Lemon': 1}\n"
     ]
    }
   ],
   "source": [
    "# Function to count the frequency of each type of a label in the dataset\n",
    "\n",
    "def count(datast):\n",
    "    count = {}  # A dict with key : value as label : freq\n",
    "    for row in datast:\n",
    "        label = row[-1]   # Since the above dataset has labels in the last column only\n",
    "        if label not in count:\n",
    "            count[label] = 0\n",
    "        count[label] += 1\n",
    "    return count\n",
    "\n",
    "# Example\n",
    "print(count(toy_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Is Diameter >= 3 ?"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Class that asks the question that best splits the dataset\n",
    "\n",
    "class Ques:\n",
    "    def __init__(self, column, value):\n",
    "        self.column = column\n",
    "        self.value = value\n",
    "    \n",
    "    # Function to check if the value passed (ex) is to be split into True or False branch based on the question asked.\n",
    "    # Example to demonstrate use is given later\n",
    "    def match(self, ex):\n",
    "        val = ex[self.column]\n",
    "        if type(val) == int:\n",
    "            return val >= self.value     # Because one of the type of questions can be if diameter is >= 3\n",
    "        else:\n",
    "            return val == self.value     # Because another type of questions can be if color matches a particular color\n",
    "    \n",
    "    # Function to print out the question formulated by values passed in __init__() in a readable format\n",
    "    def __repr__(self):\n",
    "        det_cond = \"==\"           # Partitioning condition in the tree splitting\n",
    "        if type(self.value) == int:\n",
    "            det_cond = \">=\"\n",
    "        return \"Is %s %s %s ?\" % (head[self.column], det_cond, str(self.value))\n",
    "    \n",
    "# Examples to show printing of splitting questions and the True/False branching [match() method]\n",
    "\n",
    "qs = Ques(1, 3)\n",
    "qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qs.match(toy_dataset[3])      \n",
    "# self.column = 1. So when we provide ex = toy_dataset[3], val becomes ex[3][1].It is = 3. Hence, True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True branch : [['Red', 1, 'Grape'], ['Red', 1, 'Grape']] \n",
      "\n",
      "False branch : [['Green', 3, 'Apple'], ['Yellow', 3, 'Apple'], ['Yellow', 3, 'Lemon']]\n"
     ]
    }
   ],
   "source": [
    "''' Function to partition the dataset into True branch end and False branch end. \n",
    "                                 _____________\n",
    "                                |starting_node|   best split question\n",
    "                                      /   \\\n",
    "                            True     /     \\   False\n",
    "                                    /       \\\n",
    "                                Next node   Next node\n",
    "                                   or          or\n",
    "                                Leaf node   Leaf node     ''' \n",
    "\n",
    "def partition(rows, ques):\n",
    "    true, false = [], []\n",
    "    for row in rows:\n",
    "        if ques.match(row):\n",
    "            true.append(row)\n",
    "        else:\n",
    "            false.append(row)\n",
    "    return true, false\n",
    "\n",
    "# Example of a split\n",
    "T, F = partition(toy_dataset, Ques(0, 'Red'))\n",
    "print(\"True branch :\", T,\"\\n\\nFalse branch :\", F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial impurity (label based, before split):  0.6399999999999999\n"
     ]
    }
   ],
   "source": [
    "# Calculating the Gini impurity of list of rows\n",
    "\n",
    "def gini_imp(rows):\n",
    "    cnts = count(rows)\n",
    "    impurity = 1\n",
    "    for labl in cnts:\n",
    "        p = cnts[labl]/float(len(rows))\n",
    "        impurity -= p**2\n",
    "    return impurity\n",
    "\n",
    "# Example to show calculation of Gini Impurity\n",
    "print(\"Initial impurity (label based, before split): \", gini_imp(toy_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37333333333333324"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Information Gain = uncertainty of starting node - (weighted impurity of the two child nodes)\n",
    "\n",
    "    Simply, information gain(or just Gain) = G(before split) - sum(weight * G(after split))\n",
    "    \n",
    "Calculating the information gain of a split. High Gain = most likely split'''\n",
    "\n",
    "def info_gain(left_child, right_child, before_split):\n",
    "    weight = float(len(left_child))/(len(left_child) + len(right_child))\n",
    "    return before_split - weight*gini_imp(left_child) - (1-weight)*gini_imp(right_child)\n",
    "\n",
    "true, false = partition(toy_dataset, Ques(0, 'Red'))\n",
    "info_gain(true, false, gini_imp(toy_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Red', 1, 'Grape'], ['Red', 1, 'Grape']] \n",
      "\n",
      " [['Green', 3, 'Apple'], ['Yellow', 3, 'Apple'], ['Yellow', 3, 'Lemon']]\n"
     ]
    }
   ],
   "source": [
    "print(true, \"\\n\\n\", false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.37333333333333324, Is Diameter >= 3 ?)\n"
     ]
    }
   ],
   "source": [
    "''' And finally. The function that does the best splitting by iterating repetitively over all features to see the \n",
    "    possible questions that can be asked and asking that question that gives the highest info gain'''\n",
    "\n",
    "def best_split(rows):\n",
    "    initial_uncertainty = gini_imp(rows)\n",
    "    # No. of coulmns\n",
    "    n = len(rows[0]) - 1\n",
    "    # Keep track of best information gain\n",
    "    best_gain = 0\n",
    "    # Keep track of the question that gave the best information gain\n",
    "    best_question = None\n",
    "    \n",
    "    for column in range(n):\n",
    "        values = set([row[column] for row in rows])\n",
    "        for v in values:\n",
    "            question = Ques(column, v)\n",
    "            true, false = partition(rows, question)\n",
    "            \n",
    "            if len(true) == 0 or len(false) == 0:\n",
    "                continue\n",
    "            inf_gain = info_gain(true, false, gini_imp(rows))\n",
    "            if inf_gain >= best_gain:\n",
    "                best_gain, best_question = inf_gain, question\n",
    "    return best_gain, best_question\n",
    "\n",
    "\n",
    "# Example to find splitting question of starting node of our dataset\n",
    "print(best_split(toy_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' A class to define the leaf nodes of a tree. A leaf node is basically the count of a particular label at a specific row \n",
    "    from the training data that satisfies the conditions to be a leaf node.'''\n",
    "\n",
    "class Leaf:\n",
    "    def __init__(self, rows):\n",
    "        self.predictions = count(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Now, the class to create a splitting node or the Decsision Node'''\n",
    "\n",
    "class Dec_node:\n",
    "    def __init__(self, question, true, false):\n",
    "        self.question = question\n",
    "        self.true = true\n",
    "        self.false = false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Finally, the we write the function to build the tree. First we do the start split, to decide the root of the tree. \n",
    "    Obviously it is split into the True and False branches. Then, we recursively build the tree on true branch and the false\n",
    "    branch. This is continued till information gain at the node are = 0. They are assigned as Leaf nodes.'''\n",
    "\n",
    "def build_Tree(rows):\n",
    "    # We are finding the first best split question to zero down on the root node.\n",
    "    i_gain, ques = best_split(rows)\n",
    "    # Next we see if information gain is zero. If yes, no split occurs, Leaf Node is assigned. (Base condition)\n",
    "    \n",
    "    if i_gain == 0:\n",
    "        return Leaf(rows)\n",
    "    \n",
    "    # However, if gain is not zero, we split the dataset into the true and false branches. The nodes at the end of the branches\n",
    "    # become decision nodes...the function build_Tree is again called recursively on these decision nodes.\n",
    "  \n",
    "    true_rows, false_rows = partition(rows, ques)\n",
    "    \n",
    "    true_node = build_Tree(true_rows)\n",
    "    false_node = build_Tree(false_rows)\n",
    "    \n",
    "    return Dec_node(ques, true_node, false_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Now we write the function to print the tree. '''\n",
    "\n",
    "def print_Tree(node, spacing = \"\"):\n",
    "    # First we check the base condition,i.e, if we've reached a Leaf\n",
    "    if isinstance(node, Leaf):\n",
    "        print(spacing + \"Predict\", node.predictions)\n",
    "        return\n",
    "    \n",
    "    # If not a leaf node,\n",
    "    \n",
    "    print(spacing + str(node.question))\n",
    "    \n",
    "    # The True branch\n",
    "    print(spacing + \"--> True\")\n",
    "    print_Tree(node.true, spacing + \" \")\n",
    "    \n",
    "    # The False branch\n",
    "    print(spacing + \"--> False\")\n",
    "    print_Tree(node.false, spacing + \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Diameter >= 3 ?\n",
      "--> True\n",
      " Is Color == Yellow ?\n",
      " --> True\n",
      "  Predict {'Apple': 1, 'Lemon': 1}\n",
      " --> False\n",
      "  Predict {'Apple': 1}\n",
      "--> False\n",
      " Predict {'Grape': 2}\n"
     ]
    }
   ],
   "source": [
    "this_tree = build_Tree(toy_dataset)\n",
    "print_Tree(this_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Creating the classifier '''\n",
    "\n",
    "def classify(row, node):\n",
    "    if isinstance(node, Leaf):\n",
    "        return node.predictions\n",
    "    if node.question.match(row):\n",
    "        return classify(row, node.true)\n",
    "    else:\n",
    "        return classify(row, node.false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Apple': 1}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify(toy_dataset[0], this_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Apple': '50%', 'Lemon': '50%'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def print_l(counts):\n",
    "    \"\"\" Print the predictions at a leaf, in percentage \"\"\"\n",
    "    total = sum(counts.values()) * 1.0\n",
    "    probs = {}\n",
    "    for lbl in counts.keys():\n",
    "        probs[lbl] = str(int(counts[lbl] / total * 100)) + \"%\"\n",
    "    return probs\n",
    "\n",
    "# Example\n",
    "print_l(classify(toy_dataset[1], this_tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: Grape. Predicted: {'Grape': '100%'}\n",
      "Actual: Lemon. Predicted: {'Apple': '50%', 'Lemon': '50%'}\n",
      "Actual: Grape. Predicted: {'Grape': '100%'}\n",
      "Actual: Apple. Predicted: {'Apple': '50%', 'Lemon': '50%'}\n",
      "Actual: Lemon. Predicted: {'Apple': '50%', 'Lemon': '50%'}\n",
      "Actual: Apple. Predicted: {'Apple': '100%'}\n",
      "Actual: Grape. Predicted: {'Grape': '100%'}\n",
      "Actual: Lemon. Predicted: {'Apple': '50%', 'Lemon': '50%'}\n"
     ]
    }
   ],
   "source": [
    "# Now we use this self-made decision true on a test sample for prediction\n",
    "\n",
    "test_data = [['Red', 1, 'Grape'], ['Yellow', 3, 'Lemon'], ['Red', 1, 'Grape'], ['Yellow', 3, 'Apple'], ['Yellow', 3, 'Lemon'], ['Green', 3, 'Apple'], ['Red', 1, 'Grape'], ['Yellow', 3, 'Lemon']]\n",
    "\n",
    "for row in test_data:\n",
    "    print(\"Actual: %s. Predicted: %s\" % (row[-1], print_l(classify(row, this_tree))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Decision Tree classifier is pre-written in Python. But by making and working our way through one, we got to understand the\\n    actual calculations and mechanism of working of a decision tree. '"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Decision Tree classifier is pre-written in Python. But by making and working our way through one, we got to understand the\n",
    "    actual calculations and mechanism of working of a decision tree. '''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
